{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_6B_sol.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NqwsuF_f8Sz",
        "colab_type": "text"
      },
      "source": [
        "#Assignment 6B\n",
        "The Model using function functional api having normal, spatial sperable, depthwise seperable and grouped convolution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bf-7H_ainU2",
        "colab_type": "text"
      },
      "source": [
        "Import all requirements(Input, merge, SeperableConv2D, Concatenate are new inclusions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuPhH4x3fvXd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "% matplotlib inline\n",
        "np.random.seed(2017) \n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D,SeparableConv2D,DepthwiseConv2D,Conv3D\n",
        "#from keras.layers.SeparableConv2D import SeparableConv2D\n",
        "#from keras.layers.DepthwiseConv2D import depth2d\n",
        "from keras.layers import Activation, Flatten, Dense, Dropout\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Reshape,Input,Lambda\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.layers import Concatenate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuYne5_uio2y",
        "colab_type": "text"
      },
      "source": [
        "Import the cifar dataset and store in the variables for training and test purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHpnoCHZfO8g",
        "colab_type": "code",
        "outputId": "0a220c57-360e-492a-ff4c-ec00fe0e2281",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "(xtrain, ytrain), (xtest, ytest) = cifar10.load_data()\n",
        "img_height, img_width, channel = xtrain.shape[1],xtrain.shape[2],xtrain.shape[3]\n",
        "num_classes = len(np.unique(ytest))\n",
        "print(num_classes)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuAOhe5ijCv1",
        "colab_type": "text"
      },
      "source": [
        "Function accuracy is defined to calculate the validation accuracy by finding total no of images classfied correctly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJMT4rjgfdZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(test_x, test_y, model):\n",
        "    result = model.predict(test_x)\n",
        "    predicted_class = np.argmax(result, axis=1)\n",
        "    true_class = np.argmax(test_y, axis=1)\n",
        "    num_correct = np.sum(predicted_class == true_class) \n",
        "    accuracy = float(num_correct)/result.shape[0]\n",
        "    return (accuracy * 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSO5vjgjjELc",
        "colab_type": "text"
      },
      "source": [
        "Pixel Normalisation is done for training and testing data. And the class labels for training and testing data is converted into one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5c5nDvxm6zR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xtrain = xtrain.astype('float32')/255\n",
        "xtest = xtest.astype('float32')/255\n",
        "# convert class labels to binary class labels\n",
        "ytrain = np_utils.to_categorical(ytrain, num_classes)\n",
        "ytest = np_utils.to_categorical(ytest, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Jyc4SQFG7ad",
        "colab_type": "text"
      },
      "source": [
        "<h3>Block1 Normal Convolution</h3>\n",
        "2 layers of normal 3x3 convolution is add and after that bottleneck layer of 1x1 and maxpooling is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMFFkh_WhxmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = Input(shape=(img_height, img_width, channel,))\n",
        "\n",
        "# BLOCK 1\n",
        "L11 = Conv2D(32, (3,3), strides=(1,1), name='normalconv1', use_bias=False,padding='same')(input)\n",
        "L11 = BatchNormalization()(L11)\n",
        "L11 = Activation('relu')(L11)\n",
        "\n",
        "L12 = Conv2D(64, (3,3), strides=(1,1), name='normalconv2', use_bias=False)(L11)\n",
        "L12 = BatchNormalization()(L12)\n",
        "L12 = Activation('relu')(L12)\n",
        "\n",
        "L13 = Conv2D(16,1,use_bias=False)(L12)\n",
        "L13 = MaxPooling2D(pool_size=(2, 2))(L13)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL06Y2KOhyYK",
        "colab_type": "text"
      },
      "source": [
        "#Block2 - Spatially Seperable Convolution\n",
        "2 layers of Spatially Separable Convolution(3x1 and 1x3) is add and after that bottleneck layer of 1x1 and maxpooling is used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSu38f6PG8Fy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "L21 = Conv2D(16, (3,1), strides=(1,1), name='spatialseperable1-3-1', use_bias=False)(L13)\n",
        "L21 = BatchNormalization()(L21)\n",
        "L21 = Activation('relu')(L21)\n",
        "L21 = Conv2D(32, (1,3), strides=(1,1), name='spatialseperable1-1-3', use_bias=False)(L21)\n",
        "L21 = BatchNormalization()(L21)\n",
        "L21 = Activation('relu')(L21)\n",
        "\n",
        "L22 = Conv2D(32, (3,1), strides=(1,1), name='spatialseperable2-3-1', use_bias=False)(L21)\n",
        "L22 = BatchNormalization()(L22)\n",
        "L22 = Activation('relu')(L22)\n",
        "L22 = Conv2D(64, (1,3), strides=(1,1), name='spatialseperable2-1-3', use_bias=False)(L22)\n",
        "L22 = BatchNormalization()(L22)\n",
        "L22 = Activation('relu')(L22)\n",
        "\n",
        "L23 = Conv2D(32,1,use_bias=False)(L22)\n",
        "L23 = MaxPooling2D(pool_size=(2, 2))(L23)\n",
        "          \n",
        "          \n",
        "          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dliTpOc8m6zw",
        "colab_type": "text"
      },
      "source": [
        "#Block3 - Depthwise Separable Convolution\n",
        "1 Depthwise Separable Convolution layer is apllied which first seperate all the input channel and then apply 3x3 on each individually and then 1x1 is applied to combine them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XthxbmL9nP3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "L31 = DepthwiseConv2D(3,3)(L23)\n",
        "L31 = SeparableConv2D(64, (3,3), strides=(1, 1), depth_multiplier=1, use_bias=False, name='depthwise-seperable')(L23)\n",
        "L31 = BatchNormalization()(L31)\n",
        "L31 = Activation('relu')(L31)\n",
        "#L31.shape()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBtyy2CXn72x",
        "colab_type": "text"
      },
      "source": [
        "#Block4 - Grouped Convolution (use 3x3, 5x5 only)\n",
        "The group convolution means 2 diffent set of convolution is applied on input layer in parallel. 3x3 is applied on L3. and parallely 2 3x3 are applied , and then output of both convolution are concatneted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmXpFJYzoE6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "L41 = Conv2D(128, (3,3), strides=(1,1), name='3x3', use_bias=False)(L31)\n",
        "L41 = BatchNormalization()(L41)\n",
        "L41 = Activation('relu')(L41)\n",
        "\n",
        "L42 = Conv2D(64, (3,3), strides=(1,1), name='5x5-1',padding='same', use_bias=False)(L31)\n",
        "L42 = BatchNormalization()(L42)\n",
        "L42 = Activation('relu')(L42)\n",
        "L42 = Conv2D(128, (3,3), strides=(1,1), name='5x5-2', use_bias=False)(L42)\n",
        "L42 = BatchNormalization()(L42)\n",
        "L42 = Activation('relu')(L42)\n",
        "\n",
        "L4 = concatenate([L41,L42])\n",
        "\n",
        "L4 = Conv2D(32,1)(L4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4sM4lhnod2X",
        "colab_type": "text"
      },
      "source": [
        "#Block5 - Grouped Convolution (3x3 with dilation of 1 and 2)\n",
        "The group convolution means 2 diffent set of convolution is applied on input layer in parallel. 3x3 with dilation 1 is applied on L4. and parallely 3x3 with dilation of 2 is applied , and then output of both convolution are concatneted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mia374z9olFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "L51 = Conv2D(64, (3,3), strides=(1,1), name='dilation-1',padding='same', use_bias=False,dilation_rate=(1, 1))(L4)\n",
        "L51 = BatchNormalization()(L51)\n",
        "L51 = Activation('relu')(L51)\n",
        "\n",
        "L52 = Conv2D(64, (3,3), strides=(1,1), name='dilation-2',padding='same', use_bias=False,dilation_rate=(2, 2))(L4)\n",
        "L52 = BatchNormalization()(L52)\n",
        "L52 = Activation('relu')(L52)\n",
        "\n",
        "L5 = concatenate([L51,L52])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz_IigVkpAiy",
        "colab_type": "text"
      },
      "source": [
        "Finally the no of channels are reduced to 10 and flatten and softmax is applied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0velOkH3pE3D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OUTPUT = Conv2D(10,1)(L5)\n",
        "OUTPUT = Flatten()(OUTPUT)\n",
        "Output = Activation('softmax')(OUTPUT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXxjBm6upN6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the model\n",
        "model= Model(inputs=[input],outputs=[Output])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwikLXqfpRQm",
        "colab_type": "text"
      },
      "source": [
        "Model is compiled and summary is printed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5fbBjB8G8hV",
        "colab_type": "code",
        "outputId": "8eafb80f-0c90-44d8-da75-18e8b5386e62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1768
        }
      },
      "source": [
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "normalconv1 (Conv2D)            (None, 32, 32, 32)   864         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32, 32, 32)   128         normalconv1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 32, 32, 32)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "normalconv2 (Conv2D)            (None, 30, 30, 64)   18432       activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 30, 30, 64)   256         normalconv2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 30, 30, 64)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 30, 30, 16)   1024        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 15, 15, 16)   0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "spatialseperable1-3-1 (Conv2D)  (None, 13, 15, 16)   768         max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 13, 15, 16)   64          spatialseperable1-3-1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 13, 15, 16)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "spatialseperable1-1-3 (Conv2D)  (None, 13, 13, 32)   1536        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 13, 13, 32)   128         spatialseperable1-1-3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 13, 13, 32)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "spatialseperable2-3-1 (Conv2D)  (None, 11, 13, 32)   3072        activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 11, 13, 32)   128         spatialseperable2-3-1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 11, 13, 32)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "spatialseperable2-1-3 (Conv2D)  (None, 11, 11, 64)   6144        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 11, 11, 64)   256         spatialseperable2-1-3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 11, 11, 64)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 11, 11, 32)   2048        activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 5, 5, 32)     0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "depthwise-seperable (SeparableC (None, 3, 3, 64)     2336        max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 3, 3, 64)     256         depthwise-seperable[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 3, 3, 64)     0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "5x5-1 (Conv2D)                  (None, 3, 3, 64)     36864       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 3, 3, 64)     256         5x5-1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 3, 3, 64)     0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "3x3 (Conv2D)                    (None, 1, 1, 128)    73728       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "5x5-2 (Conv2D)                  (None, 1, 1, 128)    73728       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 1, 1, 128)    512         3x3[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 1, 1, 128)    512         5x5-2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 1, 1, 128)    0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 1, 1, 128)    0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 1, 1, 256)    0           activation_22[0][0]              \n",
            "                                                                 activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 1, 1, 32)     8224        concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dilation-1 (Conv2D)             (None, 1, 1, 64)     18432       conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dilation-2 (Conv2D)             (None, 1, 1, 64)     18432       conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 1, 1, 64)     256         dilation-1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 1, 1, 64)     256         dilation-2[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 1, 1, 64)     0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 1, 1, 64)     0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 1, 1, 128)    0           activation_25[0][0]              \n",
            "                                                                 activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 1, 1, 10)     1290        concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 10)           0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 10)           0           flatten_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 269,930\n",
            "Trainable params: 268,426\n",
            "Non-trainable params: 1,504\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOLr80S4pwpl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3488
        },
        "outputId": "ba3778f4-932f-4dc6-a25e-9696f76821af"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(zoom_range=0.0, \n",
        "                             horizontal_flip=False)\n",
        "\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
        "# train the model\n",
        "start = time.time()\n",
        "# Train the model\n",
        "filepath=\"saved2.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "model_info = model.fit_generator(datagen.flow(xtrain, ytrain, batch_size = 128),\n",
        "                                 samples_per_epoch = xtrain.shape[0], nb_epoch = 50, \n",
        "                                 validation_data = (xtest,ytest),callbacks= callbacks_list, verbose=1)\n",
        "end = time.time()\n",
        "print (\"Model took %0.2f seconds to train\"%(end - start))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=(array([[[..., callbacks=[<keras.ca..., verbose=1, steps_per_epoch=390, epochs=50)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "390/390 [==============================] - 13s 34ms/step - loss: 1.5285 - acc: 0.4474 - val_loss: 1.7871 - val_acc: 0.4027\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.40270, saving model to saved2.hdf5\n",
            "Epoch 2/50\n",
            "390/390 [==============================] - 10s 26ms/step - loss: 1.1818 - acc: 0.5771 - val_loss: 1.5321 - val_acc: 0.4969\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.40270 to 0.49690, saving model to saved2.hdf5\n",
            "Epoch 3/50\n",
            "390/390 [==============================] - 10s 27ms/step - loss: 1.0214 - acc: 0.6366 - val_loss: 1.0912 - val_acc: 0.6145\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.49690 to 0.61450, saving model to saved2.hdf5\n",
            "Epoch 4/50\n",
            "390/390 [==============================] - 10s 26ms/step - loss: 0.9091 - acc: 0.6757 - val_loss: 1.2322 - val_acc: 0.5784\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.61450\n",
            "Epoch 5/50\n",
            "390/390 [==============================] - 10s 26ms/step - loss: 0.8283 - acc: 0.7060 - val_loss: 1.0916 - val_acc: 0.6174\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.61450 to 0.61740, saving model to saved2.hdf5\n",
            "Epoch 6/50\n",
            "390/390 [==============================] - 10s 26ms/step - loss: 0.7617 - acc: 0.7278 - val_loss: 0.9868 - val_acc: 0.6624\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.61740 to 0.66240, saving model to saved2.hdf5\n",
            "Epoch 7/50\n",
            "390/390 [==============================] - 11s 27ms/step - loss: 0.7017 - acc: 0.7476 - val_loss: 1.1121 - val_acc: 0.6334\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.66240\n",
            "Epoch 8/50\n",
            "390/390 [==============================] - 11s 27ms/step - loss: 0.6404 - acc: 0.7718 - val_loss: 1.0967 - val_acc: 0.6414\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.66240\n",
            "Epoch 9/50\n",
            "390/390 [==============================] - 11s 27ms/step - loss: 0.5876 - acc: 0.7917 - val_loss: 0.9379 - val_acc: 0.6853\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.66240 to 0.68530, saving model to saved2.hdf5\n",
            "Epoch 10/50\n",
            "390/390 [==============================] - 10s 26ms/step - loss: 0.5408 - acc: 0.8074 - val_loss: 0.9188 - val_acc: 0.6947\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.68530 to 0.69470, saving model to saved2.hdf5\n",
            "Epoch 11/50\n",
            "390/390 [==============================] - 10s 26ms/step - loss: 0.4972 - acc: 0.8231 - val_loss: 0.9713 - val_acc: 0.6872\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.69470\n",
            "Epoch 12/50\n",
            "390/390 [==============================] - 10s 26ms/step - loss: 0.4499 - acc: 0.8398 - val_loss: 1.0095 - val_acc: 0.6854\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.69470\n",
            "Epoch 13/50\n",
            "390/390 [==============================] - 10s 26ms/step - loss: 0.4065 - acc: 0.8568 - val_loss: 1.0331 - val_acc: 0.6837\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.69470\n",
            "Epoch 14/50\n",
            "390/390 [==============================] - 11s 27ms/step - loss: 0.3702 - acc: 0.8675 - val_loss: 1.0746 - val_acc: 0.6880\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.69470\n",
            "Epoch 15/50\n",
            "390/390 [==============================] - 10s 26ms/step - loss: 0.3251 - acc: 0.8834 - val_loss: 1.0297 - val_acc: 0.7037\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.69470 to 0.70370, saving model to saved2.hdf5\n",
            "Epoch 16/50\n",
            "390/390 [==============================] - 10s 25ms/step - loss: 0.3001 - acc: 0.8912 - val_loss: 1.2568 - val_acc: 0.6696\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.70370\n",
            "Epoch 17/50\n",
            "390/390 [==============================] - 10s 26ms/step - loss: 0.2757 - acc: 0.9003 - val_loss: 1.3114 - val_acc: 0.6760\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.70370\n",
            "Epoch 18/50\n",
            "390/390 [==============================] - 10s 25ms/step - loss: 0.2506 - acc: 0.9095 - val_loss: 1.2372 - val_acc: 0.6876\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.70370\n",
            "Epoch 19/50\n",
            "390/390 [==============================] - 10s 25ms/step - loss: 0.2252 - acc: 0.9182 - val_loss: 1.3255 - val_acc: 0.6781\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.70370\n",
            "Epoch 20/50\n",
            "390/390 [==============================] - 10s 25ms/step - loss: 0.1999 - acc: 0.9279 - val_loss: 1.3637 - val_acc: 0.6866\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.70370\n",
            "Epoch 21/50\n",
            "390/390 [==============================] - 10s 25ms/step - loss: 0.1924 - acc: 0.9310 - val_loss: 1.4452 - val_acc: 0.6803\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.70370\n",
            "Epoch 22/50\n",
            "390/390 [==============================] - 10s 25ms/step - loss: 0.1863 - acc: 0.9336 - val_loss: 1.3729 - val_acc: 0.6929\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.70370\n",
            "Epoch 23/50\n",
            "390/390 [==============================] - 11s 28ms/step - loss: 0.1649 - acc: 0.9404 - val_loss: 1.4045 - val_acc: 0.6928\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.70370\n",
            "Epoch 24/50\n",
            "390/390 [==============================] - 17s 43ms/step - loss: 0.1553 - acc: 0.9442 - val_loss: 1.3780 - val_acc: 0.6926\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.70370\n",
            "Epoch 25/50\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.1409 - acc: 0.9497 - val_loss: 1.4545 - val_acc: 0.6872\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.70370\n",
            "Epoch 26/50\n",
            "390/390 [==============================] - 15s 39ms/step - loss: 0.1416 - acc: 0.9483 - val_loss: 1.4033 - val_acc: 0.7049\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.70370 to 0.70490, saving model to saved2.hdf5\n",
            "Epoch 27/50\n",
            "390/390 [==============================] - 16s 40ms/step - loss: 0.1350 - acc: 0.9522 - val_loss: 1.5648 - val_acc: 0.6807\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.70490\n",
            "Epoch 28/50\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.1267 - acc: 0.9547 - val_loss: 1.4872 - val_acc: 0.6944\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.70490\n",
            "Epoch 29/50\n",
            "390/390 [==============================] - 21s 53ms/step - loss: 0.1332 - acc: 0.9519 - val_loss: 1.5168 - val_acc: 0.6864\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.70490\n",
            "Epoch 30/50\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.1179 - acc: 0.9578 - val_loss: 1.5411 - val_acc: 0.6866\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.70490\n",
            "Epoch 31/50\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.1165 - acc: 0.9584 - val_loss: 1.5632 - val_acc: 0.6947\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.70490\n",
            "Epoch 32/50\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.0986 - acc: 0.9656 - val_loss: 1.6744 - val_acc: 0.6870\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.70490\n",
            "Epoch 33/50\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.1084 - acc: 0.9608 - val_loss: 1.6961 - val_acc: 0.6857\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.70490\n",
            "Epoch 34/50\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.1139 - acc: 0.9590 - val_loss: 1.6837 - val_acc: 0.6882\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.70490\n",
            "Epoch 35/50\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.1025 - acc: 0.9635 - val_loss: 1.5593 - val_acc: 0.6989\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.70490\n",
            "Epoch 36/50\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.0942 - acc: 0.9677 - val_loss: 1.6147 - val_acc: 0.6926\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.70490\n",
            "Epoch 37/50\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.0926 - acc: 0.9664 - val_loss: 1.6626 - val_acc: 0.6923\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.70490\n",
            "Epoch 38/50\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.0884 - acc: 0.9691 - val_loss: 1.7751 - val_acc: 0.6777\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.70490\n",
            "Epoch 39/50\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.0929 - acc: 0.9667 - val_loss: 1.6915 - val_acc: 0.6888\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.70490\n",
            "Epoch 40/50\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.0885 - acc: 0.9690 - val_loss: 1.6784 - val_acc: 0.6944\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.70490\n",
            "Epoch 41/50\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.0831 - acc: 0.9706 - val_loss: 1.7283 - val_acc: 0.6983\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.70490\n",
            "Epoch 42/50\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.0969 - acc: 0.9659 - val_loss: 1.6902 - val_acc: 0.6969\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.70490\n",
            "Epoch 43/50\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.0759 - acc: 0.9737 - val_loss: 1.6953 - val_acc: 0.6931\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.70490\n",
            "Epoch 44/50\n",
            "390/390 [==============================] - 20s 51ms/step - loss: 0.0825 - acc: 0.9700 - val_loss: 1.7767 - val_acc: 0.6928\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.70490\n",
            "Epoch 45/50\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.0850 - acc: 0.9702 - val_loss: 1.7758 - val_acc: 0.6875\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.70490\n",
            "Epoch 46/50\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.0714 - acc: 0.9739 - val_loss: 1.6883 - val_acc: 0.6955\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.70490\n",
            "Epoch 47/50\n",
            "390/390 [==============================] - 20s 53ms/step - loss: 0.0747 - acc: 0.9738 - val_loss: 1.7752 - val_acc: 0.7001\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.70490\n",
            "Epoch 48/50\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.0717 - acc: 0.9753 - val_loss: 1.7835 - val_acc: 0.6964\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.70490\n",
            "Epoch 49/50\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.0838 - acc: 0.9702 - val_loss: 1.8308 - val_acc: 0.6873\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.70490\n",
            "Epoch 50/50\n",
            "390/390 [==============================] - 20s 52ms/step - loss: 0.0711 - acc: 0.9749 - val_loss: 1.7302 - val_acc: 0.6954\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.70490\n",
            "Model took 779.02 seconds to train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbz-9Mt_G9LF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "da8173a0-13a8-47da-803a-4d8ee3e1d59f"
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('saved2.hdf5')\n",
        "score = model.evaluate(xtest, ytest, verbose=0)\n",
        "print(score)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.4032960188865662, 0.7049]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umA8z1V-G9Pz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KaCKeRNG9Xg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR15X0MNG9f4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PatUwH9NG9x1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cflIBbCuG9Jy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8d8nelQG8_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoQGcd55G86A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44nuDnZiG80N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}